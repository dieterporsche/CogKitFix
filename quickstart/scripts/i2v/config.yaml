# ================ Logging ================
name4train: "i2v-train"
log_level: "INFO"  # Options: ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]

# ================ Model ================
model_name: "cogvideox1.5-i2v"  # Options: ["cogvideox-i2v", "cogvideox1.5-i2v"]
model_path: "${env:MODEL_PATH}"



# ================ Output ================
output_dir: "${env:OUTPUT_DIR}"


# ================ Tracker ================
report_to: null  # Options: ["wandb"]



# ================ Data ================
data_root: "${env:DATA_ROOT}"


# ================ Training ================
seed: 42
training_type: "lora"   # Options: ["lora", "sft"]

strategy: "DDP"  # Options: ["DDP", "SHARD_GRAD_OP", "FULL_SHARD", "HYBRID_SHARD", "_HYBRID_SHARD_ZERO2"]

# This will offload model param and grads to CPU memory to save GPU memory, but will slow down training
offload_params_grads: false


no_grad_sync_when_accumulating: false


enable_packing: false


train_resolution: [17, 768, 768]  # [Frames, Height, Width]

train_epochs: 2
batch_size: 1
gradient_accumulation_steps: 1
mixed_precision: "bf16"  # Options: ["fp32", "fp16", "bf16"]
learning_rate: 5.0e-5

num_workers: 32
pin_memory: true

checkpointing_steps: 0
checkpointing_limit: 0
resume_from_checkpoint: null  # or "/path/to/checkpoint/dir"


# ================ Validation ================
do_validation: true
validation_steps: 0  # Must be a multiple of `checkpointing_steps`
gen_fps: 16
